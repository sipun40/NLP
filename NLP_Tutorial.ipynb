{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/5BhUtmHGyJKLzbT/MZPi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipun40/NLP/blob/main/NLP_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbnR4W9yJwG3",
        "outputId": "08a3315d-182d-4a53-c8a8-4845239d3b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found a match am\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"i am a data scientist\"\n",
        "pattern =\"am\"\n",
        "\n",
        "match = re.search(pattern,text)\n",
        "\n",
        "if match:\n",
        "  print(\"found a match\" , match.group())\n",
        "else:\n",
        "  print(\"match not found\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am a data analyst\"\n",
        "pattern =\"analyst\"\n",
        "replacement = \"scientist\"\n",
        "new_text= re.sub(pattern,replacement,text)\n",
        "print(\"orginal text\",text)\n",
        "print(\"New Text:\",new_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2Rfubf1K1kU",
        "outputId": "8c4a5b6d-bdcf-452b-8676-a2ac388f302f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orginal text I am a data analyst\n",
            "New Text: I am a data scientist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = \"Hey, this is best: for! us;\"\n",
        "print(\"The orginal string is:\"+test1)\n",
        "res = re.sub(r'[^\\w\\s]','',test1)\n",
        "print(\"The string after punctuation filter:\"+ res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64rukteaLf5Y",
        "outputId": "637e4b96-8b39-4adc-83d8-825c2ac67f84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The orginal string is:Hey, this is best: for! us;\n",
            "The string after punctuation filter:Hey this is best for us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLKT - NATURAL LANGUAGE TOOL KIT\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JvC7mn14NKQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWhCiQdLOYlc",
        "outputId": "cf65472e-9e01-4333-acc0-b0b859170375"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # Fixed typo: 'nlkt' should be 'nltk'\n",
        "nltk.download('punkt_tab')\n",
        "text = \"This is  an example sentence. Another sentence.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens) # Removed extra indentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwExD3taN7SE",
        "outputId": "eae091c7-25af-4a15-f50d-0868dfcdff6a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', '.', 'Another', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"This  is the first  sentence. This is the  second sentence.And this is third sentence.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHx_uNBtPvno",
        "outputId": "048b4124-d5be-4fe6-fb32-b8d211250266"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This  is the first  sentence.', 'This is the  second sentence.And this is third sentence.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util  import ngrams\n",
        "text = \"This is an example sentence.\"\n",
        "token = nltk.word_tokenize(text)\n",
        "print(\"orginaln Text:\",text)\n",
        "print(\"Tokens:\",tokens)\n",
        "bigrams = list(ngrams(tokens,2))\n",
        "print(list(bigrams))\n",
        "nltk.trigrams = list(ngrams(tokens,3))\n",
        "print(list(nltk.trigrams))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhKeZfHkQBN7",
        "outputId": "21afd416-8586-4572-e0a9-0984ee3f4b65"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orginaln Text: This is an example sentence.\n",
            "Tokens: ['This', 'is', 'an', 'example', 'sentence', '.', 'Another', 'sentence', '.']\n",
            "[('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'sentence'), ('sentence', '.'), ('.', 'Another'), ('Another', 'sentence'), ('sentence', '.')]\n",
            "[('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'sentence'), ('example', 'sentence', '.'), ('sentence', '.', 'Another'), ('.', 'Another', 'sentence'), ('Another', 'sentence', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer # Fixed typo: 'nlkt' should be 'nltk'\n",
        "stemmer = PorterStemmer()\n",
        "words = \"running\"\n",
        "stemmed_word = stemmer.stem(words) # Fixed variable name: 'word' should be 'words'\n",
        "print(stemmed_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PGnIZy2SjBh",
        "outputId": "7eaa2e04-1230-4f53-d9fd-aa1a3f44c372"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk # Fixed typo: 'nlkt' should be 'nltk'\n",
        "nltk.download('omw-1.4') # Changed 'nlkt' to 'nltk'\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') # Changed 'nlkt' to 'nltk'\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = \"running\"\n",
        "lemmatized_word = lemmatizer.lemmatize(words)\n",
        "print(lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA9nXFvITGpA",
        "outputId": "8345fd62-0ea9-4d57-8df7-850b7979a9ac"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords # Changed 'nlkt' to 'nltk'\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = \"This is an example sentence with some stop words.\"\n",
        "tokens= nltk.word_tokenize(text)\n",
        "filtered_tokens= [word for word in tokens if not  word.lower() in stop_words]\n",
        "filtered_text = ' '.join(filtered_tokens)\n",
        "print(\"data\",filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WARlalBkVGxP",
        "outputId": "48197300-1399-4d18-9d8b-b9c2cb339ee6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "data example sentence stop words .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "from nltk.tokenize import word_tokenize # Corrected typo: 'nlkt' to 'nltk'\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Corrected typo: 'nlkt' to 'nltk'\n",
        "text = \"This quick brown fox jumps over the lazy dog.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUdult5jWlTY",
        "outputId": "b4cb3f53-13f3-4e15-e528-c689a54013a1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FNER\n",
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "# Define the text\n",
        "text=(\"Apple is expected to launch its new iPhone in September at the Steve Jobs in cupertion, callifarnia\")\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "# Perform part-of-speech tagging on the tokens\n",
        "tagged= pos_tag(tokens)\n",
        "#Perfors named entity recognition on the tagged text\n",
        "ne= ne_chunk(tagged)\n",
        "# Print the named entities\n",
        "for chunk in ne:\n",
        "    if hasattr(chunk, 'label'):\n",
        "         print(chunk.label(),' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRzmi__CZQTg",
        "outputId": "4949f32f-6f9c-4adc-ddce-5a2adc16ce15"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE Apple\n",
            "PERSON Steve Jobs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer # Corrected import statement\n",
        "nltk.download('vader_lexicon') # Corrected download string\n",
        "# Initialize the sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "# Sample text\n",
        "\n",
        "text =\"I hate this product! It woutld not work perfectly and I would definitely recommend it to anyone.\"\n",
        "# Compute the sentiment scores for the text scores sid.polarity_scores (text)\n",
        "scores = sid.polarity_scores(text)\n",
        "# Print the sentiment scores\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJqDWYhXciSV",
        "outputId": "dcfe6dfb-b6be-455a-ecf5-54af1b0e37f8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "{'neg': 0.312, 'neu': 0.467, 'pos': 0.221, 'compound': -0.4871}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}